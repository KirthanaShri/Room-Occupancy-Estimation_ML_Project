# -*- coding: utf-8 -*-
"""ML Project - Room Occupancy Estimation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17X8QpidE2m8bDQ3T4YPRiwd3P4v67k9K
"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE
from sklearn.preprocessing import LabelEncoder
import datetime
from sklearn.preprocessing import LabelEncoder
from tqdm import tqdm
import math
from scipy import stats
from scipy.stats import norm
from scipy.special import softmax
from sklearn.metrics import classification_report
import plotly.graph_objects as grp
from  dataclasses import dataclass
import keras
from sklearn.preprocessing import OneHotEncoder
from scipy import optimize
from sklearn.metrics import multilabel_confusion_matrix
import plotly.subplots as sp
import plotly.graph_objects as go
import missingno as msno
from pandas.core.common import random_state
from scipy import optimize
from sklearn import svm
from scipy.optimize import Bounds, minimize
from os import supports_effective_ids

url = "https://archive.ics.uci.edu/ml/machine-learning-databases/00640/Occupancy_Estimation.csv"
df = pd.read_csv(url)
df = pd.DataFrame(df)
df

num_cols = ['S1_Temp','S2_Temp','S3_Temp','S4_Temp','S1_Light','S2_Light','S3_Light','S4_Light','S1_Sound','S2_Sound','S3_Sound','S4_Sound','S5_CO2','S5_CO2_Slope']
cat_cols = ['S6_PIR','S7_PIR','Room_Occupancy_Count']

"""# Exploratory Data Analysis"""

df.shape

"""## Data types of columns"""

df.dtypes

"""## Checking for Missing Values"""

df.isna().sum()

df.describe()

"""## Unique values in each column"""

df.nunique()

"""## Seasonality check"""

df['Date'].unique()

# Group by date and count the number of records for each day
daily_counts = df.groupby('Date').size().reset_index(name='count')
# Plotting
plt.figure(figsize=(10, 6))
plt.plot(daily_counts['Date'], daily_counts['count'], marker='o', linestyle='-', color='b')
plt.title('Number of Records per Day')
plt.xlabel('Date')
plt.ylabel('Number of Records')
plt.grid(True)
plt.show()

df['Date_time'] = pd.to_datetime(df['Date'] +' '+ df['Time'])
dt_time = df.pop('Date_time')
df.insert(2, 'Date_time', dt_time)

df['Hours'] = df['Date_time'].dt.hour
df['Time_of_Day'] = pd.cut(df['Hours'], bins = [0,6,12,17,22,24], labels = ['Night','Morning','Afternoon','Evening','Night'], include_lowest=True, ordered = False)

df.pop('Hours')
day_time = df.pop('Time_of_Day')
df.insert(2, 'Time_of_Day', day_time)

plt.figure(figsize=(12,7))
ax = sns.countplot(data = df,x = 'Room_Occupancy_Count', hue = 'Time_of_Day', hue_order = ['Morning','Afternoon','Evening','Night'])
for p in ax.patches:
    ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()),
                ha='center', va='center', xytext=(0, 10), textcoords='offset points')
plt.show()

# Group by date and time_of_day, calculate average room occupancy
avg_occupancy = df.groupby(['Date_time', 'Time_of_Day'])['Room_Occupancy_Count'].mean().reset_index()

# Pivot the DataFrame for easier plotting
pivot_df = avg_occupancy.pivot(index='Date_time', columns='Time_of_Day', values='Room_Occupancy_Count')

# Plot lines for each time of day
for time_of_day in pivot_df.columns:
    plt.plot(pivot_df.index, pivot_df[time_of_day], label=time_of_day)

plt.title('Average Room Occupancy for Morning, Afternoon, and Evening')
plt.xlabel('Date')
plt.ylabel('Average Room Occupancy')
plt.legend(title='Time of Day', loc='upper left', bbox_to_anchor=(1, 1))
plt.show()

sns.scatterplot(data = df, x = 'S1_Light', y = 'S1_Sound', hue = 'Room_Occupancy_Count', palette = ['Red','Green','Blue','Yellow'])
plt.show()

"""# Data is linearly separable

## Boxplot showing range and outliers for each numerical column
"""

for i in num_cols:
  plt.figure(figsize=(15,2))
  sns.boxplot(data = df, x = i)
  plt.title('Boxplot for {}'.format(i))

"""## Correlation between numerical features"""

plt.figure(figsize=(13,7))
sns.heatmap(df[num_cols].corr(),annot = True)
plt.title('Heatmap showing correlation between columns')
plt.show()

"""## Count of Room Occupancy"""

plt.figure(figsize=(10,7))
ax = sns.countplot(data = df,x = 'Room_Occupancy_Count')
for p in ax.patches:
    ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()),
                ha='center', va='center', xytext=(0, 10), textcoords='offset points')
plt.show()

value_counts = df['Room_Occupancy_Count'].value_counts()

percentages = (value_counts / value_counts.sum()) * 100

plt.figure(figsize=(8, 8))
plt.pie(percentages, labels=percentages.index, autopct='%1.1f%%', startangle=90)
plt.title('Percentage distribution of Room_Occupancy_Count')
plt.show()

"""# Data Pre-processing"""

label_encoder = LabelEncoder()
df['Time_of_Day'] = label_encoder.fit_transform(df['Time_of_Day'])

"""## Removing columns with high correlation (> 0.9) and also removing redundant columns"""

df1 = df.copy()

df1.drop(columns=['Date','Time','Date_time','S1_Temp','S3_Temp'],axis=1,inplace=True)

df1.head()

"""## Functions for splitting data and standardization"""

def split_data(data):

  X = data.loc[:, data.columns!='Room_Occupancy_Count']
  y = data['Room_Occupancy_Count']
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=13, stratify = y)

  X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=13, stratify = y_train)

  return X_train, X_val, X_test, y_train, y_val, y_test

def standardize_data(X_train, X_val, X_test):
  mean = np.mean(X_train,axis=0)
  std = np.std(X_train,axis=0)

  X_train = (X_train - mean) / std
  X_val = (X_val - mean) / std
  X_test = (X_test - mean) / std

  return X_train, X_val, X_test

"""## Baseline Model evaluation"""

X_dummy_train, X_dummy_val, X_dummy_test, y_dummy_train, y_dummy_val, y_dummy_test = split_data(df1)
X_dummy_train, X_dummy_val, X_dummy_test = standardize_data(X_dummy_train, X_dummy_val, X_dummy_test)

from sklearn.metrics import precision_score, recall_score, f1_score

class RandomClassifier:

    def __init__(self, num_classes):
        self.num_classes = num_classes

    def predict(self, X):
        return np.random.randint(low=0, high=self.num_classes, size=X.shape[0])

def baseline_accuracy(y_true, y_pred):
    correct = np.sum(y_true == y_pred)
    total = len(y_true)
    return correct / total

def baseline_evaluate(y_true, y_pred):
    acc = round((baseline_accuracy(y_true, y_pred))*100,2)
    precision = round((precision_score(y_true, y_pred, average='weighted'))*100,2)
    recall = round((recall_score(y_true, y_pred, average='weighted'))*100,2)
    f1 = round((f1_score(y_true, y_pred, average='weighted'))*100,2)

    print(f"Accuracy : {acc}")
    print(f"Precision : {precision}")
    print(f"Recall : {recall}")
    print(f"F1 Score : {f1}")

random_model = RandomClassifier(4)

y_true = y_dummy_test.values
y_pred_random = random_model.predict(X_dummy_test.values)

baseline_evaluate(y_true, y_pred_random)

# Class for calculating Principal components and returning transformed dataframe with PCA applied
class PCA():

# Initialization of required parameters of class
    def __init__(self, X):
        self.X = X
        self.num_pcas = None
        self.mean = None
        self.std = None
        self.eigen_val = None
        self.eigen_vec = None

# Function to fit data and return principal components
    def fit(self):
        self.mean = np.mean(self.X, axis=0)
        self.std = np.std(self.X, axis=0)
# Normalizing dataframe
        X_std = (self.X - self.mean) / self.std
# Calculating covariance matrix of normalized dataframe
        cov_matrix = np.cov(X_std, rowvar=False)

# Using eigen value decomposition to get eigen values and eigen vectors for the matrix
        self.eigen_val, self.eigen_vec = np.linalg.eig(cov_matrix)

# Sorting eigen vectors in decreasing order of eigen values
        sorted_eigen = np.argsort(self.eigen_val)[::-1]
        self.eigen_val = self.eigen_val[sorted_eigen]
        self.eigen_vec = self.eigen_vec[:, sorted_eigen]

# Calculating total cumulative captured variance
        cum_var = []
        for i in range(1, len(self.eigen_val) + 1):
            pcas = self.eigen_val[0: i]
            cum_var.append(pcas.sum() / self.eigen_val.sum())
            print("Variance captured by {} Principal Components is {}%".format(i, round(pcas.sum() / self.eigen_val.sum() * 100, 2)))

# Function to transform dataframe based on number of components selected
    def transform(self, X, num_comp):

        top_eigenvec = self.eigen_vec[:, :num_comp]

        X_standard = (X - self.mean) / self.std

        X_pca = np.dot(X_standard, top_eigenvec)

        return X_pca

"""#Logistic regression"""

# Class for Logistic Regression
class LogisticRegression:

# Initialization of required parameters of class
    def __init__(self, df, output_column, learning_rate, max_iteration, epsilon, regularization_parameter,num_pcas):
        self.df = df
        self.output_column = output_column
        self.learning_rate = learning_rate
        self.max_iteration = max_iteration
        self.epsilon = epsilon
        self.regularization_parameter = regularization_parameter
        self.num_pcas = num_pcas
        self.theta = None
        self.train_errors = []
        self.val_errors = []

# Function for splitting data into Train (60%), Validation (20%) and Test (20%)
    def split_data(self):
        X = self.df.drop('{}'.format(self.output_column), axis=1)
        y = self.df['{}'.format(self.output_column)]
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=13, stratify = y)
        X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=13, stratify = y_train)
        return X_train, X_val, X_test, y_train, y_val, y_test

# Normalizing train dataset using standardization formula: (X - mean) / standard deviation
    def normalize_train(self, X):
        mean = np.mean(X, axis=0)
        std = np.std(X, axis=0)
        X_normalized = (X - mean) / std
        return X_normalized, mean, std

# Normalizing validation and test sets using mean and standard deviation value from Train set
    def normalize_test_val(self, X, mean, std):
        X_normalized = (X - mean) / std
        return X_normalized

# Function to handle undersampling of majority class to the total size of all minority classes
    def undersample(self, X, y):
        class_values = y.value_counts()
        min_classes = class_values[class_values == class_values.min()].index

        max_class_values = y[~y.isin(min_classes)].index
        min_class_values = y[y.isin(min_classes)].index

        sample_max = np.random.choice(max_class_values, size=len(min_class_values), replace=False)

        X_sampled = pd.concat([X.loc[sample_max], X.loc[min_class_values]])
        y_sampled = pd.concat([y.loc[sample_max], y.loc[min_class_values]])

        return X_sampled, y_sampled

# Using SMOTE to oversample on minority classes
    def oversample(self, X, y):
        smote = SMOTE(random_state=42)
        X_sampled, y_sampled = smote.fit_resample(X, y)
        return X_sampled, y_sampled

# Softmax function calculated
    def softmax(self, X):
        e_scores = np.exp(np.array(X) - np.max(np.array(X), axis=1, keepdims=True))
        sm = e_scores / np.sum(e_scores, axis=1, keepdims=True)
        return sm

# Total error/ cost function for softmax
    def cost_function_softmax(self, X, y):
        prob = self.softmax(X.dot(self.theta))
        y_hat = (np.arange(prob.shape[1]) == y.to_numpy()[:, None]).astype(float)
        cost = -1/X.shape[0] * np.sum(y_hat * np.log(prob))
        reg_term = (self.regularization_parameter / (2 * X.shape[0])) * np.sum(self.theta**2)
        return (np.mean(cost + reg_term))/2

# Gradient of cost function
    def cost_derivative_softmax(self, X, y):
        prob = self.softmax(X.dot(self.theta))
        y_hat = (np.arange(prob.shape[1]) == y.to_numpy()[:, None]).astype(float)
        grad = 1/X.shape[0] * (X.T @ (prob - y_hat))
        reg_term = (self.regularization_parameter / X.shape[0]) * self.theta
        reg_term[0] = 0
        return (grad + reg_term)

# Function for gradient descent and calculation of Training vs Validation error plot
    def gradient_descent(self, X_train, y_train, X_val, y_val):
        for i in tqdm(range(self.max_iteration)):
            self.theta -= self.learning_rate * self.cost_derivative_softmax(X_train, y_train)
            t_error = self.cost_function_softmax(X_train, y_train)
            v_error = self.cost_function_softmax(X_val, y_val)
            self.train_errors.append(t_error)
            self.val_errors.append(v_error)

            if len(self.train_errors) > 1:
              if abs(self.train_errors[-2] - t_error) < self.epsilon:
                  print("Stopped Learning")
                  break

        self.plot_cost()

# Using softmax function to predict class with max probability
    def predict(self, X):
        X = np.column_stack([np.ones([X.shape[0], 1]), X])
        return np.argmax(self.softmax(X.dot(self.theta)), axis=1)

# Evaluation function to calculate Accuracy, Precision, Recall and F1 score
    def evaluate(self, X, y):
        predictions = self.predict(X)
        accuracy = np.mean(predictions == y)

     # Creating confusion matrix
        confusion_matrix = np.zeros((len(np.unique(y)), len(np.unique(y))))
        for i in range(len(y)):
            confusion_matrix[y.iloc[i], predictions[i]] += 1

        precision = np.mean(np.diag(confusion_matrix) / np.sum(confusion_matrix, axis=1))
        recall = np.mean(np.diag(confusion_matrix) / np.sum(confusion_matrix, axis=0))
        f1_sc = 2 * np.mean((precision * recall) / (precision + recall))

        print(f"Accuracy: {accuracy}")
        print(f"Precision: {precision}")
        print(f"Recall: {recall}")
        print(f"F1 Score: {f1_sc}")

        return accuracy, precision, recall, f1_sc

# Finally calling fit function to process all the data and return evaluation metrics
    def fit(self):
        X_train, X_val, X_test, y_train, y_val, y_test = self.split_data()
        X_train, mean, std = self.normalize_train(X_train)
        X_val = self.normalize_test_val(X_val, mean, std)
        X_test = self.normalize_test_val(X_test, mean, std)

        X_train, y_train = self.undersample(X_train, y_train)
        X_train, y_train = self.oversample(X_train, y_train)

# If PCA is to be applied o dataframe
        if self.num_pcas:
            pca = PCA(X_train)
            pca.fit()
            X_train = pca.transform(X_train,self.num_pcas)
            X_val = pca.transform(X_val,self.num_pcas)
            X_test = pca.transform(X_test,self.num_pcas)

# Adding bias term to train and validation datasets
        X_train = np.column_stack([np.ones([X_train.shape[0], 1]), X_train])
        X_val = np.column_stack([np.ones([X_val.shape[0], 1]), X_val])

        # Initializing theta values
        self.theta = np.zeros((X_train.shape[1], len(np.unique(y_train))))

        self.gradient_descent(X_train, y_train, X_val, y_val)
        accuracy, precision, recall, f1_sc = self.evaluate(X_test,y_test)
        return accuracy, precision, recall, f1_sc

# Line plot for Training vs Validation error
    def plot_cost(self):
        plt.plot(self.train_errors, label='Training Error')
        plt.plot(self.val_errors, label='Validation Error')
        plt.xlabel('Iteration')
        plt.ylabel('Cost')
        plt.title('Training vs Validation Error')
        plt.legend()
        plt.show()

"""## Logistic Regression with PCA"""

lr1 = LogisticRegression(df1, 'Room_Occupancy_Count', learning_rate=0.1, max_iteration=2000, epsilon=0.00001, regularization_parameter=0.1,num_pcas=8)

accuracy, precision, recall, f1_sc = lr1.fit()

# Initializing range of values for learning rate, tolerance and regularization parameter
learning_rate = [0.001,0.01,0.1]
epsilon = [0.0001,0.00001,0.000001]
reg_param = [0.01,0.1,1]

# Creating empty dictionary to store values
results_dict = {'Learning Rate': [], 'Tolerance': [], 'Reg. Parameter': [], 'Accuracy': [], 'Precision': [], 'Recall': [], 'F1 Score': []}

# Iterating through all hyperparameter values and running Logistic Regression with PCA for each combination
for i in learning_rate:
  for j in epsilon:
    for k in reg_param:
      lr = LogisticRegression(df1, 'Room_Occupancy_Count', learning_rate=i, max_iteration=2000, epsilon=j, regularization_parameter=k, num_pcas=8)
      accuracy, precision, recall, f1_sc = lr.fit()
      results_dict['Learning Rate'].append(i)
      results_dict['Tolerance'].append(j)
      results_dict['Reg. Parameter'].append(k)
      results_dict['Accuracy'].append(accuracy)
      results_dict['Precision'].append(precision)
      results_dict['Recall'].append(recall)
      results_dict['F1 Score'].append(f1_sc)

# Creating dataframe of the results
LR_pca_df = pd.DataFrame(results_dict)

# Converting evaluation scores to % and rounding off
LR_pca_df['Accuracy'] = round(LR_pca_df['Accuracy']*100,2)
LR_pca_df['Precision'] = round(LR_pca_df['Precision']*100,2)
LR_pca_df['Recall'] = round(LR_pca_df['Recall']*100,2)
LR_pca_df['F1 Score'] = round(LR_pca_df['F1 Score']*100,2)

# Displaying final results sorted by Precision score
LR_pca_df.sort_values('Precision',ascending=False)

"""## Logistic Regression without PCA"""

lr = LogisticRegression(df1, 'Room_Occupancy_Count', learning_rate=0.01, max_iteration=2000, epsilon=0.00001, regularization_parameter=0.1,num_pcas=None)

accuracy, precision, recall, f1_sc = lr.fit()

# Initializing range of values for learning rate, tolerance and regularization parameter
learning_rate = [0.001,0.01,0.1]
epsilon = [0.0001,0.00001,0.000001]
reg_param = [0.01,0.1,1]

# Creating empty dictionary to store values
results_dict = {'Learning Rate': [], 'Tolerance': [], 'Reg. Parameter': [], 'Accuracy': [], 'Precision': [], 'Recall': [], 'F1 Score': []}

# Iterating through all hyperparameter values and running Logistic Regression for each combination
for i in learning_rate:
  for j in epsilon:
    for k in reg_param:
      lr = LogisticRegression(df1, 'Room_Occupancy_Count', learning_rate=i, max_iteration=2000, epsilon=j, regularization_parameter=k, num_pcas=None)
      accuracy, precision, recall, f1_sc = lr.fit()
      results_dict['Learning Rate'].append(i)
      results_dict['Tolerance'].append(j)
      results_dict['Reg. Parameter'].append(k)
      results_dict['Accuracy'].append(accuracy)
      results_dict['Precision'].append(precision)
      results_dict['Recall'].append(recall)
      results_dict['F1 Score'].append(f1_sc)

# Creating dataframe of the results
LR_df = pd.DataFrame(results_dict)

# Converting evaluation scores to % and rounding off
LR_df['Accuracy'] = round(LR_df['Accuracy']*100,2)
LR_df['Precision'] = round(LR_df['Precision']*100,2)
LR_df['Recall'] = round(LR_df['Recall']*100,2)
LR_df['F1 Score'] = round(LR_df['F1 Score']*100,2)

# Displaying final results sorted by Precision score
LR_df.sort_values('Precision',ascending=False)

"""#Feedforward Neural Network"""

import tensorflow as tf
y = df1['Room_Occupancy_Count']
X = df1.loc[:, df1.columns!='Room_Occupancy_Count']
X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size = 0.1,
                                                  random_state= 0, stratify= y)
X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp,
                                                  test_size = 0.2,
                                                  random_state= 0,
                                                  stratify= y_temp)

X_train = pd.DataFrame(X_train)
X_val = pd.DataFrame(X_val)
X_test = pd.DataFrame(X_test)
y_train = pd.DataFrame(y_train)
y_val = pd.DataFrame(y_val)
y_test = pd.DataFrame(y_test)

def NN_Model(lr, reg_para, act_fn):

    tf.random.set_seed(42)
    # Defining a deep fully connected neural network model
    model_nn = tf.keras.Sequential([
    tf.keras.layers.Dense(X_train.shape[1], input_dim = X_train.shape[1], activation=act_fn, kernel_regularizer=tf.keras.regularizers.l2(reg_para)),
    tf.keras.layers.Dense(15, activation=act_fn, kernel_regularizer=tf.keras.regularizers.l2(reg_para)),
    tf.keras.layers.Dense(4, activation='softmax')
      ])


    # Compiling the model
    model_nn.compile(optimizer= tf.keras.optimizers.Adam(learning_rate=lr),
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])


    # Printing model summary
    # model_nn.summary()

    history = model_nn.fit(X_train, y_train, epochs=20, batch_size=32, validation_data = (X_val, y_val), shuffle = True)

    # Evaluating the model on the test set
    test_loss, test_accuracy = model_nn.evaluate(X_test, y_test, batch_size=32)

    # Predicting the classes for the test set
    y_pred_probs = model_nn.predict(X_test)

    # Obtaining the predicted class labels by selecting the class with the highest probability
    y_pred = y_pred_probs.argmax(axis=1)

    # Calculating precision, recall, and F1-score
    precision = precision_score(y_test, y_pred, average='weighted')
    recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)
    f1 = f1_score(y_test, y_pred, average='weighted')
    report = classification_report(y_test, y_pred)
    print(report)

    return history, test_accuracy, precision, recall, f1

#Evaluating performance of the model - for each combination of hyperparameters
lr_values = [0.01,0.001,0.0001]
reg_para_values = [0.01,0.001,0.005]
act_fns = ['relu', 'sigmoid', 'tanh', 'LeakyReLU']

#Running the model with evaluation added to dataframe
df_acc = pd.DataFrame()
for lr in lr_values:
  for reg_para in reg_para_values:
    for act_fn in act_fns:
      print(f' Learning rate: {lr} | Regularisation parameter: {reg_para} | Activation function: {act_fn}')
      history , test_acc, precision, recall, f1_ = NN_Model(lr,reg_para,act_fn)

      df_acc = df_acc.append({'lr': lr, 'reg_para': reg_para, 'act_fn': act_fn, 'test_accuracy': test_acc, 'history':history,
                              'precision': precision, 'recall': recall, 'F1-score': f1_}, ignore_index=True)

#Plotting between train and validation error for the configuration with highest model performance
df_acc = df_acc.sort_values('precision', ascending=False)

plt.plot(df_acc.at[0,'history'].history['loss'], label='Train')
plt.plot(df_acc.at[0,'history'].history['val_loss'], label='Validation')
plt.title("Train error vs Validation error")
plt.legend()

# Highlighting the highest performance of the mode
df_acc.sort_values('precision', ascending=False).drop(columns='history').style.apply(lambda x: ['background: green' if i==0 else '' for i in range(len(x))], axis=0)

from tabulate import tabulate
print(tabulate(df_acc.sort_values('precision', ascending=False).drop(columns='history'), headers='keys', tablefmt='psql'))

"""#SVM"""

df1

df_svm=df1

# Extract features and target variable
target_col = 'Room_Occupancy_Count'
y = df_svm[target_col]
X = df_svm.drop(columns=[target_col])
numerical_cols = X.select_dtypes(include=np.number).columns

# Split the dataset
X_train_raw, X_test_raw, y_train_raw, y_test_raw = train_test_split(X, y, test_size=0.25, random_state=0, stratify=y)
X_train, X_val, y_train, y_val = train_test_split(X_train_raw, y_train_raw, test_size=0.2, random_state=0, stratify=y_train_raw)

# Undersample the training set
df_train_combined = pd.concat([X_train, y_train], axis=1)
undersampled_majority = df_train_combined[df_train_combined[target_col] == 0].sample(n=1000, replace=True, random_state=0)
df_undersampled = pd.concat([undersampled_majority, df_train_combined[df_train_combined[target_col] != 0]], ignore_index=True)

y_train = df_undersampled[target_col]
X_train = df_undersampled.drop(columns=[target_col])

# Oversample the training set
smote = SMOTE(random_state=42)
X_train, y_train = smote.fit_resample(X_train, y_train)

# Standardize the data
numerical_cols = X_train.select_dtypes(include=np.number).columns
for column in numerical_cols:
    mean = np.mean(X_train[column], 0)
    std = np.std(X_train[column], 0)
    X_train[column] = (X_train[column] - mean) / std
    X_val[column] = (X_val[column] - mean) / std
    X_test_raw[column] = (X_test_raw[column] - mean) / std

# Convert target variables to DataFrame
y_train = pd.DataFrame(y_train)
y_val = pd.DataFrame(y_val)
y_test_raw = pd.DataFrame(y_test_raw)

# Display the shapes of the datasets
print(f"X_train shape: {X_train.shape}, X_val shape: {X_val.shape}, X_test shape: {X_test_raw.shape}")
print(f"y_train shape: {y_train.shape}, y_val shape: {y_val.shape}, y_test shape: {y_test_raw.shape}")

"""##Without PCA Transformation"""

# Original train-test split
X_train_first, X_test_first, y_train_first, y_test_first = train_test_split(X_train, y_train, test_size=0.3,
                                                      random_state=0,
                                                      stratify=y_train)

# Further splits
X_train_second, X_test_second, y_train_second, y_test_second = train_test_split(X_train_first, y_train_first, test_size=0.3,
                                                      random_state=0,
                                                      stratify=y_train_first)

X_train_svm, X_test_svm, y_train_svm, y_test_svm = train_test_split(X_train_second, y_train_second, test_size=0.3,
                                                      random_state=0,
                                                      stratify=y_train_second)

X_train_s, X_test_s, y_train_s, y_test_s = train_test_split(X_train_svm, y_train_svm, test_size=0.3,
                                                      random_state=0,
                                                      stratify=y_train_svm)

# X_train_s, X_test_s, y_train_s, y_test_s = train_test_split(X_train_third, y_train_third, test_size=0.3,
#                                                       random_state=0,
#                                                       stratify=y_train_third)

# Convert to numpy arrays for X_train_svm
X_train_svm, X_test_svm, y_train_svm, y_test_svm = map(lambda x: x.to_numpy(), [X_train_svm, X_test_svm, y_train_svm, y_test_svm])

# Define additional y variables for y_train_svm
y0_train, y1_train, y2_train, y3_train = [np.where(y_train_svm == i, 1, -1) for i in range(4)]

# Convert to numpy arrays for X_train_s
X_train_s, X_test_s, y_train_s, y_test_s = map(lambda x: x.to_numpy(), [X_train_s, X_test_s, y_train_s, y_test_s])

# Define additional y variables for y_train_s
y0_train_s, y1_train_s, y2_train_s, y3_train_s = [np.where(y_train_s == i, 1, -1) for i in range(4)]


# Print statements with variable names
print("First Split - Shapes:", X_train_first.shape, X_test_first.shape, y_train_first.shape, y_test_first.shape)
print("Second Split - Shapes:", X_train_second.shape, X_test_second.shape, y_train_second.shape, y_test_second.shape)
print("SVM Split - Shapes:", X_train_svm.shape, X_test_svm.shape, y_train_svm.shape, y_test_svm.shape)
# print("Third Split - Shapes:", X_train_third.shape, X_test_third.shape, y_train_third.shape, y_test_third.shape)
print("Final Split (S) - Shapes:", X_train_s.shape, X_test_s.shape, y_train_s.shape, y_test_s.shape)

from os import supports_effective_ids
@dataclass
class SoftMarginSVM2:

  C: float

  def dualSVM(self,alpha, gramXy):
    return alpha.sum() - 0.5 * alpha.dot(alpha.dot(gramXy))


  def jacobian_dualSVM(self,alpha, gramXy):
    return np.ones(len(alpha)) - alpha.dot(gramXy)


  def fit(self, X, y):

    N, n_features = X.shape

    y = y.reshape(-1, 1)

    Xy = X * y
    gramXy = np.matmul(Xy, Xy.T)

    alphas = np.zeros(N)
    alpha_bounds = Bounds(np.zeros(N), np.ones(N) * self.C)

    constraints = ({'type':'eq',
                    'fun': lambda alpha: np.dot(alpha, y).item(),
                    'jac': lambda alpha: -y.flatten()})

    slsqp = optimize.minimize(fun = lambda a: -self.dualSVM(a, gramXy),
                              x0 = alphas,
                              jac = lambda a: -self.jacobian_dualSVM(a, gramXy),
                              constraints = constraints,
                              bounds = alpha_bounds,
                              method = 'SLSQP')

    self.alphas = slsqp.x

    self.w = np.sum(Xy.T.dot(self.alphas[:, np.newaxis]), axis=1)
    epsilon = 0.0001
    self.support_vectors = X[self.alphas > epsilon]
    self.support_labels = y[self.alphas > epsilon]

    b = []
    for i in range(len(self.support_vectors)):
      b_i = self.support_labels[i] - np.matmul(self.support_vectors[i].T, self.w.T)
      b.append(b_i)

    self.b = sum(b)/len(b)

import numpy as np
import pandas as pd
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

# Define a list of C values
C_values = [0.01, 0.1, 1.0]

# Initialize an empty DataFrame to store results
result_df = pd.DataFrame(columns=['Parameter(C)', 'Accuracy', 'Precision', 'Recall', 'F1 score'])

for C in C_values:
    print("C: {}".format(C))

    # Initialize empty lists to store results
    w_list = []
    b_list = []
    y_list = []

    # Initialize SoftMarginSVM2 for the current C value
    soft_margin = SoftMarginSVM2(C=C)

    # Fit the model for each class and store the results
    for i in range(4):  # 4 classes (0, 1, 2, 3)
        Xi_train = globals()['X_train_s']
        yi_train = globals()['y{}_train_s'.format(i)]
        Xi_test_s = globals()['X_test_s']

        soft_margin.fit(Xi_train, yi_train)
        w = soft_margin.w.reshape(15, 1)
        b = soft_margin.b

        w_list.append(w)
        b_list.append(b)

        # Make predictions on the test set
        y = Xi_test_s.dot(w) + b
        y_list.append(y)

    # Concatenate and process the results
    w_array = np.concatenate(w_list, axis=1)
    b_array = np.array(b_list)
    y_concat_s = np.concatenate(y_list, axis=1)
    max_y_s = np.argmax(y_concat_s, axis=1)

    # Use classification_report to calculate metrics
    report = classification_report(y_test_s, max_y_s, target_names=['0', '1', '2', '3'], output_dict=True)

    # Extract metrics
    accuracy = report['accuracy']
    precision = report['macro avg']['precision']
    recall = report['macro avg']['recall']
    f1_svm_score = report['macro avg']['f1-score']

    print("\tAccuracy : {}".format(accuracy))
    print("\tPrecision : {}".format(precision))
    print("\tRecall : {}".format(recall))
    print("\tF1 score : {}".format(f1_svm_score))

    # Append results to the DataFrame
    result_df = result_df.append({'Parameter(C)': C, 'Accuracy': accuracy,
                                  'Precision': precision, 'Recall': recall, 'F1 score': f1_svm_score},
                                 ignore_index=True)

    fig, ax = plt.subplots(figsize=(4, 4))
    cm = confusion_matrix(y_test_s, max_y_s)
    cmp = ConfusionMatrixDisplay(cm)
    cmp.plot(ax=ax)
    plt.show()
    print("\n\n")

# Sort the DataFrame by Precision
result_df = result_df.sort_values(by='Precision', ascending=False)

# Display the results DataFrame
print(result_df)

"""##With PCA Transformation"""

pca_svm = PCA(X_train)
pca_svm.fit()
X_train_pca = pca_svm.transform(X_train,8)
X_val_pca = pca_svm.transform(X_val,8)
X_test_pca = pca_svm.transform(X_test_raw,8)

# Original train-test split
X_train_first, X_test_first, y_train_first, y_test_first = train_test_split(X_train_pca, y_train, test_size=0.3,
                                                      random_state=0,
                                                      stratify=y_train)

# Further splits
X_train_second, X_test_second, y_train_second, y_test_second = train_test_split(X_train_first, y_train_first, test_size=0.3,
                                                      random_state=0,
                                                      stratify=y_train_first)

X_train_svm, X_test_svm, y_train_svm, y_test_svm = train_test_split(X_train_second, y_train_second, test_size=0.3,
                                                      random_state=0,
                                                      stratify=y_train_second)

X_train_s, X_test_s, y_train_s, y_test_s = train_test_split(X_train_svm, y_train_svm, test_size=0.3,
                                                      random_state=0,
                                                      stratify=y_train_svm)

# X_train_s, X_test_s, y_train_s, y_test_s = train_test_split(X_train_third, y_train_third, test_size=0.3,
#                                                       random_state=0,
#                                                       stratify=y_train_third)

# Convert to numpy arrays for X_train_svm
# X_train_svm, X_test_svm, y_train_svm, y_test_svm = map(lambda x: x.to_numpy(), [X_train_svm, X_test_svm, y_train_svm, y_test_svm])

# Define additional y variables for y_train_svm
y0_train, y1_train, y2_train, y3_train = [np.where(y_train_svm == i, 1, -1) for i in range(4)]

# Convert to numpy arrays for X_train_s
# X_train_s, X_test_s, y_train_s, y_test_s = map(lambda x: x.to_numpy(), [X_train_s, X_test_s, y_train_s, y_test_s])

# Define additional y variables for y_train_s
y0_train_s, y1_train_s, y2_train_s, y3_train_s = [np.where(y_train_s == i, 1, -1) for i in range(4)]


# Print statements with variable names
print("First Split - Shapes:", X_train_first.shape, X_test_first.shape, y_train_first.shape, y_test_first.shape)
print("Second Split - Shapes:", X_train_second.shape, X_test_second.shape, y_train_second.shape, y_test_second.shape)
print("SVM Split - Shapes:", X_train_svm.shape, X_test_svm.shape, y_train_svm.shape, y_test_svm.shape)
# print("Third Split - Shapes:", X_train_third.shape, X_test_third.shape, y_train_third.shape, y_test_third.shape)
print("Final Split (S) - Shapes:", X_train_s.shape, X_test_s.shape, y_train_s.shape, y_test_s.shape)

import numpy as np
import pandas as pd
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

# Define a list of C values
C_values = [0.01, 0.1, 1.0]

# Initialize an empty DataFrame to store results
result_df = pd.DataFrame(columns=['Parameter(C)', 'Accuracy', 'Precision', 'Recall', 'F1 score'])

for C in C_values:
    print("C: {}".format(C))

    # Initialize empty lists to store results
    w_list = []
    b_list = []
    y_list = []

    # Initialize SoftMarginSVM2 for the current C value
    soft_margin_pca = SoftMarginSVM2(C=C)

    # Fit the model for each class and store the results
    for i in range(4):  # 4 classes (0, 1, 2, 3)
        Xi_train = globals()['X_train_s']
        yi_train = globals()['y{}_train_s'.format(i)]
        Xi_test_s = globals()['X_test_s']

        soft_margin_pca.fit(Xi_train, yi_train)
        w = soft_margin_pca.w.reshape(8, 1)
        b = soft_margin_pca.b

        w_list.append(w)
        b_list.append(b)

        # Make predictions on the test set
        y = Xi_test_s.dot(w) + b
        y_list.append(y)

    # Concatenate and process the results
    w_array = np.concatenate(w_list, axis=1)
    b_array = np.array(b_list)
    y_concat_s = np.concatenate(y_list, axis=1)
    max_y_s = np.argmax(y_concat_s, axis=1)

    # Use classification_report to calculate metrics
    report = classification_report(y_test_s, max_y_s, target_names=['0', '1', '2', '3'], output_dict=True)

    # Extract metrics
    accuracy = report['accuracy']
    precision = report['macro avg']['precision']
    recall = report['macro avg']['recall']
    f1_svm_score = report['macro avg']['f1-score']

    print("\tAccuracy : {}".format(accuracy))
    print("\tPrecision : {}".format(precision))
    print("\tRecall : {}".format(recall))
    print("\tF1 score : {}".format(f1_svm_score))

    # Append results to the DataFrame
    result_df = result_df.append({'Parameter(C)': C, 'Accuracy': accuracy,
                                  'Precision': precision, 'Recall': recall, 'F1 score': f1_svm_score},
                                 ignore_index=True)

    fig, ax = plt.subplots(figsize=(4, 4))
    cm = confusion_matrix(y_test_s, max_y_s)
    cmp = ConfusionMatrixDisplay(cm)
    cmp.plot(ax=ax)
    plt.show()
    print("\n\n")

# Sort the DataFrame by Precision
result_df = result_df.sort_values(by='Precision', ascending=False)

# Display the results DataFrame
print(result_df)